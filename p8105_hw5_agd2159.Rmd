---
title: "P8015_Hw5_agd2159"
output: github_document
author: "Zander De Jesus"
date: "11-15-2023"
---

```{r Global Settings, message = FALSE}
library(tidyverse)
library(rvest)
library(patchwork)
set.seed(1234)

theme_set(theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1: Washington Post Homicide Data Exploration

```{r}
wp_homicides = read_csv("homicide-data/homicide-data.csv", na = c("", "NA", "Unknown")) |> 
  mutate(city_state = str_c(city, state, sep = ","),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved")) |> 
  filter(city_state != "Tulsa,AL")

```

Before more transformative cleaning, this dataframe has `r nrow(wp_homicides)` entries, on variables that include the victim name, race, age, and sex; the date the homicide was reported; and the location of the homicide. Using the mutate and case_when functions, we add a city_state variable that combines the string data from both city and state, and a resolution column variable that indicates the present status of a homicide case. Filtering out Tulsa, AL which was accidentally included alongside Tulsa, OK.

```{r}
totals_homicide_df = 
  wp_homicides |> 
  select(city_state, disposition, resolution) |>  
  group_by(city_state) |> 
  summarize(
    homicides_total = n(),
    homicides_unsolved = sum(resolution == "unsolved"))
```

Starting to use `prop.test` to see the proportional significance of unresolved homicides in particular cities.
```{r}
bmore_test = 
  prop.test(
    x = filter(totals_homicide_df, city_state == "Baltimore,MD") |> pull(homicides_unsolved),
    n = filter(totals_homicide_df, city_state == "Baltimore,MD") |>  pull(homicides_total)) 

broom::tidy(bmore_test) |> 
  knitr::kable(digits = 3)
```

Now that we have this function established, important to standardize across cities. Map package can place this test across all cells within the tibble.

```{r}
test_results = 
  totals_homicide_df |> 
  mutate(
    prop_tests = map2(homicides_unsolved, homicides_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) |>  
  select(-prop_tests) |> 
  unnest(tidy_tests) |> 
  select(city_state, estimate, conf.low, conf.high) |>  
  mutate(city_state = fct_reorder(city_state, estimate))
```

We then visualize the unsolved case proportion across cities using ggplot scatterpoint tools.

```{r}
test_results |>  
  mutate(city_state = fct_reorder(city_state, estimate)) |>  
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

The percentage of unsolved homicides ranges across cities from about 25% on the low end to over 60%-70% on the highest end. **Chicago, IL, New Orleans, LA, and Baltimore, MD** are the top three cities for the highest proportion of unresolved homicides. 


# Problem 2: Compiling Randomized Control Trial Dataframe using Iterative Functions (MAP)

The Randomized Control Trial Data is coming in 20 separated CSV files, 10 for the control group and 10 for the experimental. Each contains two rows of data, the 8 weeks of data and the observational values recorded weekly in the longtitudinal study.

Based on the setup of these datasets, as we import it will be important to `pivot_longer` so that week number and value are made two standardized columns rather than disaggregated rows across study participants. 

```{r Bulk Import using map function}
#Instructions Start with a dataframe containing all file names; the list.files function will help

study_participants = list.files(path = "./data", full.names = TRUE)

#Initial Import and Column Creation
longtitudinal_results = study_participants |> 
  map_dfr(read_csv, .id = "participant_id") |> 
  mutate(study_arm = print(dir(path = "./data", include.dirs = FALSE))) |> 
  mutate(study_arm = str_remove(study_arm, ".csv")) |> 
  separate(study_arm, into = c("study_arm", "subject_id")) |> 
  select(participant_id, study_arm, subject_id, everything())

longtitudinal_results |> 
  knitr::kable(digits = 2)

#Cleaning using pivot longer
longtitudinal_results = longtitudinal_results |> 
  pivot_longer(
    week_1:week_8,
    names_to = "study_week",
    values_to = "observations",
    names_prefix = "week_"
  ) |> 
  mutate(study_week = as.numeric(study_week))

longtitudinal_results |> 
  head()

```

Now that weeks and observations are aggregated into two centralized columns, there is greater ability to do grouped_by summary calculations. 

We are asked to next visualize these observations in spaghetti plots, separating by control or experimental arm, (faceting): 

```{r}
#Instructions: Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups

ggp_results = longtitudinal_results |>
  group_by(subject_id) |> 
  ggplot(aes(x = study_week, y = observations)) + 
           geom_point(alpha = 0.7, aes(color = subject_id)) +  geom_line(alpha = 0.7, aes(color = subject_id)) +
          geom_smooth() +
  labs(
    title = "Randomized Control Trial Results Over 8 Weeks, Control vs. Experimental Arms",
    x = "Week of Study",
    y = "Observational Value"
  ) +
  facet_grid(. ~ study_arm)

ggp_results
```

**Visualization Interpretation:**

This spaghetti plot highlights the observed changes in values across all 8 weeks of the RCT, grouped by the 10 subjects in both the control and experimental study arms. This visualization shows us that for the experimental group, there was a higher range of observed values and an average increase in observed values across experimental subjects, as illustated by the trendline produced by `geom_smooth()`. The average trendline started around an observed value of approximately 1.5 at week 1 and an average value of over 5 by week 8, showing a trajectory of net increase each weekly interval.  

The control group had values that were more consistently within a narrow range, and the average trendline produced by this control group had values slightly above 1 across this 8 week study period. The control group had some members also dip below 0 into negative observed value over multiple weeks, which did not occur for any in the experimental group.

# Problem 3: Simulation of One-Sample T-Test

We start with the task of setting a normally distributed model that will be iterated, of size n = 30, standard dev sigma of 5, and mean of 0.

For this problem, the seed was set at the start of the document global settings as `set.seed(1234)`.

```{r Initial data function setup before iteration}

sim_rnorm_data = function(n, mu, sigma = 5) {
    x = rnorm(n = n, mean = mu, sd = sigma)
}

sim_t_test = function(x) {
  t.test(x = x, mu = 0, conf.level = 0.95) |> 
    broom::tidy()
}
```

I will attempt to create an iterated data frame given the model design specifications using the `expand_grid` function and `map`. Starting from the template from class `simulations.rmd` worksheet. Problem 1 solutions illustrate an example using `map2.`

```{r}
#sample size n of 30
#true means across 0 to 6
#originally had sample_size and true_mu variables but changed back to n and mu for consistency with sim_rnorm_data function. Did not want to confuse variables.

sim_results_df = 
  expand_grid(
    n = 30,
    mu = 0:6,
    iter = 1:5000
  ) |> 
  mutate(
    data_df = map2(n, mu, sim_rnorm_data)) |> 
  mutate(t_test_results = map(data_df, sim_t_test)) |> 
  unnest(t_test_results)
  
sim_results_df
```

After allowing for 5,000 iterations across 7 different assigned true means (mu = 0 through 6), the simulation dataframe `sim_results_df` holds **`r nrow(sim_results_df)`** observations across **`r ncol(sim_results_df)`** column variables. The mean mu, p value, and estimate values are of primary interest for the upcoming visualizations. 

###Visualization 1: 
*Instructions: Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of mu on the x axis. Describe the association between effect size and power.* 

```{r}
power_plot = sim_results_df |> 
  group_by(mu) |> 
  mutate(test_status = case_when(
    p.value >= 0.05 ~ "Null Accepted",
    p.value < 0.05 ~ "Null Rejected"
  )) |> 
  count(test_status)

power_plot |> 
  knitr::kable(digits = 2)
```

This cleaned data table retrieves the count n of p-values below 0.05 which reject the null and p-values above 0.05 which accept the null hypothesis. Proportions can then be calculated from the following count values.

```{r}
power_plot = power_plot |> 
  filter(test_status == "Null Rejected") |> 
  mutate(power = n / 5000)

power_plot |> 
  group_by(mu) |> 
  ggplot(aes(x = mu, y = power, fill = mu)) +
  geom_col() + 
  labs(
    title = "Identifying Hypothesis Testing Power of One-Sample T-Test, 5000 Iterations",
    x = "Assigned True Mean Mu",
    y = "Statistical Testing Power (percentage of p-value <0.05)"
  ) + 
  scale_x_continuous(breaks = scales::pretty_breaks(6))

```

This visualization identifies the percentage of p-values within the simulated 5000 t-tests that were statistically significant and rejected the null hypothesis, for the 7 different assigned true means 0 through 6. Considering in the function `sim_t_test` we were asked to test from a base mean of 0 and alpha of 0.05, the true assigned mean of 0 in the simulation dataframe had the closest results, with statistical power of **0.052** after 5,000 iterations. As the true assigned mean moved higher away from the base mean of 0, the larger amount of p-values rejected the null hypothesis of presumed randomness, and staitiscal power grows. By true assigned mean of 4 - 6, the statistical power is virtually 98 - 100%, due to having the greatest deviations from the mean across the modeled dataframes.

### Visualization 2: Plots for Average Estimate in T-Test

*Instructions: 1) Make a plot showing the average estimate of muhat on the y axis and the true value of mu on the x axis. 2) Make a second plot (or overlay on the first) the average estimate of muhat only in samples for which the null was rejected on the y axis and the true value of mu on the x axis. Is the sample average of muhat across tests for which the null is rejected approximately equal to the true value of mu? Why or why not?*

```{r Estimate Avg vs. True Mean }
average_estimate_df = sim_results_df |> 
  group_by(mu) |> 
  mutate(avg_estimate_mu = mean(estimate))

average_estimate_df |> 
  ggplot(aes(x = mu, y = avg_estimate_mu, color = mu)) + geom_point() + geom_line() +
    labs(
    title = "Comparing Estimate Means with Assigned True Means in Sim. T-Test, 5000 Iterations",
    x = "Assigned True Mean Mu",
    y = "Averaged Estimate Mu from 5000 t-tests"
  ) + scale_x_continuous(breaks = scales::pretty_breaks(6))
```

This visualization shows that the overall average estimated mean from 5000 simulation t-tests is highly accurate to the true mean that it was assigned. For example, the avg. estimate mu was 0.02864 in this sample with assigned mean of 0. Similarly, the assigned sample of true mean 4 had a avg. estimate mean of 3.981539. The line is nearly straight like a y = x relationship. 



---
title: "P8015_Hw5_agd2159"
output: github_document
author: "Zander De Jesus"
date: "11-15-2023"
---

```{r Global Settings, message = FALSE}
library(tidyverse)
library(rvest)
library(patchwork)
set.seed(1234)

theme_set(theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1: Washington Post Homicide Data Exploration

```{r}
wp_homicides = read_csv("homicide-data/homicide-data.csv", na = c("", "NA", "Unknown")) |> 
  mutate(city_state = str_c(city, state, sep = ","),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved")) |> 
  filter(city_state != "Tulsa,AL")

```

Before more transformative cleaning, this dataframe has `r nrow(wp_homicides)` entries, on variables that include the victim name, race, age, and sex; the date the homicide was reported; and the location of the homicide. Using the mutate and case_when functions, we add a city_state variable that combines the string data from both city and state, and a resolution column variable that indicates the present status of a homicide case. Filtering out Tulsa, AL which was accidentally included alongside Tulsa, OK.

```{r}
totals_homicide_df = 
  wp_homicides |> 
  select(city_state, disposition, resolution) |>  
  group_by(city_state) |> 
  summarize(
    homicides_total = n(),
    homicides_unsolved = sum(resolution == "unsolved"))
```

Starting to use `prop.test` to see the proportional significance of unresolved homicides in particular cities.
```{r}
bmore_test = 
  prop.test(
    x = filter(totals_homicide_df, city_state == "Baltimore,MD") |> pull(homicides_unsolved),
    n = filter(totals_homicide_df, city_state == "Baltimore,MD") |>  pull(homicides_total)) 

broom::tidy(bmore_test) |> 
  knitr::kable(digits = 3)
```

Now that we have this function established, important to standardize across cities. Map package can place this test across all cells within the tibble.

```{r}
test_results = 
  totals_homicide_df |> 
  mutate(
    prop_tests = map2(homicides_unsolved, homicides_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) |>  
  select(-prop_tests) |> 
  unnest(tidy_tests) |> 
  select(city_state, estimate, conf.low, conf.high) |>  
  mutate(city_state = fct_reorder(city_state, estimate))
```

We then visualize the unsolved case proportion across cities using ggplot scatterpoint tools.

```{r}
test_results |>  
  mutate(city_state = fct_reorder(city_state, estimate)) |>  
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

The percentage of unsolved homicides ranges across cities from about 25% on the low end to over 60%-70% on the highest end. **Chicago, IL, New Orleans, LA, and Baltimore, MD** are the top three cities for the highest proportion of unresolved homicides. 


# Problem 2: Compiling Randomized Control Trial Dataframe using Iterative Functions (MAP)

The Randomized Control Trial Data is coming in 20 separated CSV files, 10 for the control group and 10 for the experimental. Each contains two rows of data, the 8 weeks of data and the observational values recorded weekly in the longtitudinal study.

Based on the setup of these datasets, as we import it will be important to `pivot_longer` so that week number and value are made two standardized columns rather than disaggregated rows across study participants. 

```{r Bulk Import using map function}
#Instructions Start with a dataframe containing all file names; the list.files function will help

study_participants = list.files(path = "./data", full.names = TRUE)

#Initial Import and Column Creation
longtitudinal_results = study_participants |> 
  map_dfr(read_csv, .id = "participant_id") |> 
  mutate(study_arm = print(dir(path = "./data", include.dirs = FALSE))) |> 
  mutate(study_arm = str_remove(study_arm, ".csv")) |> 
  separate(study_arm, into = c("study_arm", "subject_id")) |> 
  select(participant_id, study_arm, subject_id, everything())

longtitudinal_results |> 
  knitr::kable(digits = 2)

#Cleaning using pivot longer
longtitudinal_results = longtitudinal_results |> 
  pivot_longer(
    week_1:week_8,
    names_to = "study_week",
    values_to = "observations",
    names_prefix = "week_"
  ) |> 
  mutate(study_week = as.numeric(study_week))

longtitudinal_results |> 
  head()

```

Now that weeks and observations are aggregated into two centralized columns, there is greater ability to do grouped_by summary calculations. 

We are asked to next visualize these observations in spaghetti plots, separating by control or experimental arm, (faceting): 

```{r}
#Instructions: Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups

ggp_results = longtitudinal_results |>
  group_by(subject_id) |> 
  ggplot(aes(x = study_week, y = observations)) + 
           geom_point(alpha = 0.7 , aes(color = subject_id)) +  geom_line(alpha = 0.7, aes(color = subject_id)) +
          geom_smooth() +
  labs(
    title = "Randomized Control Trial Results Over 8 Weeks, Control vs. Experimental Arms",
    x = "Week of Study",
    y = "Observational Value"
  ) +
  facet_grid(. ~ study_arm)

ggp_results
```

**Visualization Interpretation:**

This spaghetti plot highlights the observed changes in values across all 8 weeks of the RCT, grouped by the 10 subjects in both the control and experimental study arms. This visualization shows us that for the experimental group, there was a higher range of observed values and an average increase in observed values across experimental subjects, as illustated by the trendline produced by `geom_smooth()`. The average trendline started around an observed value of approximately 1.5 at week 1 and an average value of over 5 by week 8, showing a trajectory of net increase each weekly interval.  

The control group had values that were more consistently within a narrow range, and the average trendline produced by this control group had values slightly above 1 across this 8 week study period. The control group had some members also dip below 0 into negative observed value over multiple weeks, which did not occur for any in the experimental group.

# Problem 3: Simulation of One-Sample T-Test

We start with the task of setting a normally distributed model that will be iterated, of size n = 30, standard dev sigma of 5, and mean of 0.

For this problem, the seed was set at the start of the document global settings as `set.seed(1234)`.

```{r Initial data function setup before iteration}

sim_rnorm_data = function(n, mu, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n = n, mean = mu, sd = sigma),
  )
}

sim_t_test = function(x) {
  t.test(x = x, mu = 0, conf.level = 0.95 )
}
```

I will attempt to create an iterated dataframe given the model design specifications using the `expand_grid` function and `map`. Using template from class simulations.rmd worksheet.

```{r}

sim_results_df = 
  expand_grid(
    sample_size = 30,
    mu = 0:6,
    iter = 1:100
  ) |> 
  mutate(
    estimate_df = map(sim_rnorm_data, sim_t_test)
  ) |> 
  unnest(estimate_df)
```

